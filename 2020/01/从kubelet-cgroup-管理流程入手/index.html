<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.62.2 with theme Tranquilpeak 0.4.7-BETA">
<meta name="author" content="vici">
<meta name="keywords" content="k8s, kubelet, cgroup, vici, blog, heyuhua.com, 何育华, v1c77">
<meta name="description" content="新年第一个flag: 春节期间《缺氧》能解决气体和液体供应问题。">


<meta property="og:description" content="新年第一个flag: 春节期间《缺氧》能解决气体和液体供应问题。">
<meta property="og:type" content="article">
<meta property="og:title" content="从kubelet Cgroup 管理流程入手">
<meta name="twitter:title" content="从kubelet Cgroup 管理流程入手">
<meta property="og:url" content="http://blog.heyuhua.com/2020/01/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B/">
<meta property="twitter:url" content="http://blog.heyuhua.com/2020/01/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B/">
<meta property="og:site_name" content="vici&#39;blog">
<meta property="og:description" content="新年第一个flag: 春节期间《缺氧》能解决气体和液体供应问题。">
<meta name="twitter:description" content="新年第一个flag: 春节期间《缺氧》能解决气体和液体供应问题。">
<meta property="og:locale" content="zh-cn">

  
    <meta property="article:published_time" content="2020-01-14T11:35:58">
  
  
    <meta property="article:modified_time" content="2020-01-14T11:35:58">
  
  
  
    
      <meta property="article:section" content="develop">
    
  
  


<meta name="twitter:card" content="summary">

  <meta name="twitter:site" content="@bookerHe">


  <meta name="twitter:creator" content="@bookerHe">










  <meta property="og:image" content="https://www.gravatar.com/avatar/85d1ea47d48692ca0ade206fa656bd14?s=640">
  <meta property="twitter:image" content="https://www.gravatar.com/avatar/85d1ea47d48692ca0ade206fa656bd14?s=640">


    <title>从kubelet Cgroup 管理流程入手</title>

    <link rel="icon" href="//qiniu.heyuhua.com/blog_favicon.ico">
    

    

    <link rel="canonical" href="http://blog.heyuhua.com/2020/01/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="http://blog.heyuhua.com/css/style-twzjdbqhmnnacqs0pwwdzcdbt8yhv8giawvjqjmyfoqnvazl0dalmnhdkvp7.min.css" />
    
    

    
      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-88015219-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="5">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="http://blog.heyuhua.com/">vici&#39;blog</a>
  </div>
  
    
      <a class="header-right-icon st-search-show-outputs"
         href="http://blog.heyuhua.com/#search">
    
    
      <i class="fa fa-lg fa-search"></i>
    
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="5">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="http://blog.heyuhua.com/#about">
          <img class="sidebar-profile-picture" src="https://www.gravatar.com/avatar/85d1ea47d48692ca0ade206fa656bd14?s=110" alt="作者的图片" />
        </a>
        <h4 class="sidebar-profile-name">vici</h4>
        
          <h5 class="sidebar-profile-bio">极度选择困难. <strong>懒癌晚期</strong></h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="http://blog.heyuhua.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">首页</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="http://blog.heyuhua.com/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">分类</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="http://blog.heyuhua.com/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">标签</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="http://blog.heyuhua.com/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">归档</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="http://blog.heyuhua.com/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">关于</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/v1c77" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://stackoverflow.com/users/6382035/vi-ci" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-stack-overflow"></i>
      
      <span class="sidebar-button-desc">Stack Overflow</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="http://blog.heyuhua.com/index.xml">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="5"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      从kubelet Cgroup 管理流程入手
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-01-14T11:35:58&#43;08:00">
        
  一月 14, 2020

      </time>
    
    
  
  
    <span>发布在</span>
    
      <a class="category-link" href="http://blog.heyuhua.com/categories/develop">develop</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p>新年第一个flag: 春节期间《缺氧》能解决气体和液体供应问题。</p>
<blockquote>
<p>本文基于 k8s 近期 release, git: commit 70132b0f130acc0bed193d9ba59dd186f0e634cf (HEAD, tag: v1.17.0)</p>
</blockquote>
<h2 id="前言寻找可能性------混编-vm-与-pod-cgroup">前言：寻找可能性 &mdash;- 混编 VM 与 Pod cgroup</h2>
<p>想象 k8s 可以同时管理 VM 资源了
如何将 vm 资源的cpu，内存配置抽象成 cgroup 并使用 kubelet CgroupManager 统一管理?</p>
<h2 id="kubelet-的-cgroup-管理模型">kubelet 的 Cgroup 管理模型</h2>
<p>kubelet 作为 k8s 系统中各个节点的“话事者”，其 <code>ContainerManger</code> 模块包揽了所有的 cgroup 管理工作。
ContainerManager 将 Pod 的 cgroup 模型做了层次分离： container -&gt; Pod -&gt; Qos -&gt; Node。</p>
<h3 id="node-level">Node level</h3>
<p>节点层面 主要聚焦于 cgroup 横向分配， 通过隔离不同类型 的 cgroup 进行抽象。</p>
<pre><code>                        Node Capacity
                    ---------------------------
                    |     kube-reserved       |
                    |-------------------------|
                    |     system-reserved     |
                    |-------------------------|
                    |    eviction-threshold   |
                    |-------------------------|
                    |                         |
                    |      allocatable        |
                    |   (available for pods)  |
                    |                         |
                    |                         |
                    ---------------------------
</code></pre><p>默认情况下 节点粒度的管理 分配了 k8s 服务组件的 cgroup， 系统非内核 进程 cgroup， 还有 pod 资源的 cgroup.</p>

  
    
  
  
    
  
  
    
  
  
    
  


<figure class="highlight golang language-golang">
  <figcaption>
    
      <span>server.go</span><a href="https://github.com/kubernetes/kubernetes/blob/v1.17.0/cmd/kubelet/app/server.go#L709" target="_blank" rel="external">cmd/kuelet/app/server.go</a>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-golang"><code class="golang">kubeDeps.ContainerManager, err = cm.NewContainerManager(
    kubeDeps.Mounter,
    kubeDeps.CAdvisorInterface,
    cm.NodeConfig{
        RuntimeCgroupsName:    s.RuntimeCgroups, // `runtime-cghroups`  配置, 创建和运行容器运行时的 cgroup 的绝对名称
        SystemCgroupsName:     s.SystemCgroups, // cgroup 的绝对名称，用于所有尚未放置在根目录下某 cgroup 内的非内核进程。空值表示不指定 cgroup。回滚该参数需要重启机器。
        KubeletCgroupsName:    s.KubeletCgroups, // 用于创建和运行 kubelet 的 cgroup 的绝对名称。
        ContainerRuntime:      s.ContainerRuntime,
        CgroupsPerQOS:         s.CgroupsPerQOS,  // 将 Pod 按照不同的 QOS 优先级进行 cgroup 拆分。
        CgroupRoot:            s.CgroupRoot,
        CgroupDriver:          s.CgroupDriver,
        KubeletRootDir:        s.RootDirectory,
        ProtectKernelDefaults: s.ProtectKernelDefaults,
        NodeAllocatableConfig: cm.NodeAllocatableConfig{ // 节点可分配 resource 计算结构体
            KubeReservedCgroupName:   s.KubeReservedCgroup,   // k8s 自身服务 所在 cgroup
            SystemReservedCgroupName: s.SystemReservedCgroup, // 操作系统非内核进程外的其他进程所在cgroup
            EnforceNodeAllocatable:   sets.NewString(s.EnforceNodeAllocatable...), // `enforce-node-allocatable` 参数设置，
            KubeReserved:             kubeReserved,
            SystemReserved:           systemReserved,
            ReservedSystemCPUs:       reservedSystemCPUs, // 预留给节点其他无关进程的 cpu.
            HardEvictionThresholds:   hardEvictionThresholds,
        },
        QOSReserved:                           *experimentalQOSReserved,
        ExperimentalCPUManagerPolicy:          s.CPUManagerPolicy,
        ExperimentalCPUManagerReconcilePeriod: s.CPUManagerReconcilePeriod.Duration,
        ExperimentalPodPidsLimit:              s.PodPidsLimit,
        EnforceCPULimits:                      s.CPUCFSQuota,
        CPUCFSQuotaPeriod:                     s.CPUCFSQuotaPeriod.Duration,
        ExperimentalTopologyManagerPolicy:     s.TopologyManagerPolicy,
    },
    s.FailSwapOn,
    devicePluginEnabled,
    kubeDeps.Recorder)</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>
<p>通过如上 模块设计， Pod 的 allocatable resource 可以通过简单的减法运算得出</p>
<blockquote>
<p><code>allocatable = NodeCapacity - [kube-reserved] - [system-reserved] - [eviction-threshold]</code></p>
</blockquote>
<p>我们的测试集群目前配置如下：</p>
<ul>
<li>CgroupDriver : systemd</li>
<li>SystemCgroups: /system.slice</li>
<li>KubeletCgroups: /system.slice</li>
<li>SystemReservedCgroups:  /system.slice</li>
<li>KubeReservedCgroups: /system.slice/kubelet.service</li>
</ul>
<p>额外的：</p>
<ul>
<li>VM 相关 cgroup 配置位于  /system.slice/machine.slice， 需要纳入 node的 统一管理。</li>
</ul>
<h3 id="qos-level">QOS level</h3>
<p>在上一节 Node 相关配置中， <code>--cgroup-per-qos</code> 配置（默认为 true） 会生成该层级的 Cgroup 配置。</p>
<p>目前 QOS 共分为 3 种。</p>
<p>qos 级别</p>
<ul>
<li>Guaranteed【老板（我要的都是我的）】：pod 里每个容器都必须设定 <code>request</code> 和 <code>limit</code>，并且值必须相同</li>
<li>Burstable  【洗碗工（底薪+提成）】：pod 里至少有一个容器的 cpu 或者 memory 设置了 <code>request</code> 值</li>
<li>BestEffort【切格瓦拉（能偷到的都是我的）】：POD 的所有容器都没有指定CPU和内存的 <code>request</code> 和 <code>limit</code></li>
</ul>
<p>初始化过程发生在  <code>kl.containerLogManager.Start() &gt; setupNode</code> 过程：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-golang" data-lang="golang">	<span style="color:#75715e">// Setup top level qos containers only if CgroupsPerQOS flag is specified as true
</span><span style="color:#75715e"></span>	<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">cm</span>.<span style="color:#a6e22e">NodeConfig</span>.<span style="color:#a6e22e">CgroupsPerQOS</span> {
		<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">cm</span>.<span style="color:#a6e22e">createNodeAllocatableCgroups</span>(); <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
			<span style="color:#66d9ef">return</span> <span style="color:#a6e22e">err</span>
		}
		<span style="color:#a6e22e">err</span> = <span style="color:#a6e22e">cm</span>.<span style="color:#a6e22e">qosContainerManager</span>.<span style="color:#a6e22e">Start</span>(<span style="color:#a6e22e">cm</span>.<span style="color:#a6e22e">getNodeAllocatableAbsolute</span>, <span style="color:#a6e22e">activePods</span>)
		<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
			<span style="color:#66d9ef">return</span> <span style="color:#a6e22e">fmt</span>.<span style="color:#a6e22e">Errorf</span>(<span style="color:#e6db74">&#34;failed to initialize top level QOS containers: %v&#34;</span>, <span style="color:#a6e22e">err</span>)
		}
	}
</code></pre></div><p><code>createNodeAllocatableCgroups</code> 会初始化 一个 <code>system.slice/kubepods.slice</code> cgroup, 用于放置 pod 资源</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">	<span style="color:#75715e">// Top level for Qos containers are created only for Burstable
</span><span style="color:#75715e"></span>	<span style="color:#75715e">// and Best Effort classes
</span><span style="color:#75715e"></span>	<span style="color:#a6e22e">qosClasses</span> <span style="color:#f92672">:=</span> <span style="color:#66d9ef">map</span>[<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSClass</span>]<span style="color:#a6e22e">CgroupName</span>{
		<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSBurstable</span>:  <span style="color:#a6e22e">NewCgroupName</span>(<span style="color:#a6e22e">rootContainer</span>, <span style="color:#a6e22e">strings</span>.<span style="color:#a6e22e">ToLower</span>(string(<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSBurstable</span>))),
		<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSBestEffort</span>: <span style="color:#a6e22e">NewCgroupName</span>(<span style="color:#a6e22e">rootContainer</span>, <span style="color:#a6e22e">strings</span>.<span style="color:#a6e22e">ToLower</span>(string(<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSBestEffort</span>))),
	}

    <span style="color:#75715e">// ...
</span><span style="color:#75715e"></span>
    <span style="color:#75715e">// Store the top level qos container names
</span><span style="color:#75715e"></span>	<span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">qosContainersInfo</span> = <span style="color:#a6e22e">QOSContainersInfo</span>{
		<span style="color:#a6e22e">Guaranteed</span>: <span style="color:#a6e22e">rootContainer</span>,
		<span style="color:#a6e22e">Burstable</span>:  <span style="color:#a6e22e">qosClasses</span>[<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSBurstable</span>],
		<span style="color:#a6e22e">BestEffort</span>: <span style="color:#a6e22e">qosClasses</span>[<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSBestEffort</span>],
	}
</code></pre></div><p>初始化后 <code>Burstable</code> 和 <code>BestEffort</code>类型的pod cgroup 会被生成在， <code>/system.slice/kubepods.slice</code> 下。</p>
<p>而  <code>guaranteed</code> 类型 pod 会直接 运行在  /system.slice/kubepods.slice 下.</p>
<p>这里发生了 kubelet 层的第一次  cgroup 设置： <code>BestEffort</code>其中的 cpu.shares 被设置为minShares(=2).</p>
<p>表示 在 cpu高负载情况下， BestEffort. Pod 将会享有 pod中最少的的cpu时间段。</p>
<p>同时，在containerManager start 之后， 还会有一个常驻go程   循环执行 <code>UpdateCgroups()</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#66d9ef">func</span> (<span style="color:#a6e22e">m</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">qosContainerManagerImpl</span>) <span style="color:#a6e22e">UpdateCgroups</span>() <span style="color:#66d9ef">error</span> {
	<span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">Lock</span>()
	<span style="color:#66d9ef">defer</span> <span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">Unlock</span>()

	<span style="color:#a6e22e">qosConfigs</span> <span style="color:#f92672">:=</span> <span style="color:#66d9ef">map</span>[<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSClass</span>]<span style="color:#f92672">*</span><span style="color:#a6e22e">CgroupConfig</span>{
		<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSBurstable</span>: {
			<span style="color:#a6e22e">Name</span>:               <span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">qosContainersInfo</span>.<span style="color:#a6e22e">Burstable</span>,
			<span style="color:#a6e22e">ResourceParameters</span>: <span style="color:#f92672">&amp;</span><span style="color:#a6e22e">ResourceConfig</span>{},
		},
		<span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">PodQOSBestEffort</span>: {
			<span style="color:#a6e22e">Name</span>:               <span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">qosContainersInfo</span>.<span style="color:#a6e22e">BestEffort</span>,
			<span style="color:#a6e22e">ResourceParameters</span>: <span style="color:#f92672">&amp;</span><span style="color:#a6e22e">ResourceConfig</span>{},
		},
	}

	<span style="color:#75715e">// update the qos level cgroup settings for cpu shares
</span><span style="color:#75715e"></span>	<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">setCPUCgroupConfig</span>(<span style="color:#a6e22e">qosConfigs</span>); <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
		<span style="color:#66d9ef">return</span> <span style="color:#a6e22e">err</span>
	}

	<span style="color:#75715e">// update the qos level cgroup settings for huge pages (ensure they remain unbounded)
</span><span style="color:#75715e"></span>	<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">setHugePagesConfig</span>(<span style="color:#a6e22e">qosConfigs</span>); <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
		<span style="color:#66d9ef">return</span> <span style="color:#a6e22e">err</span>
	}

	<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">utilfeature</span>.<span style="color:#a6e22e">DefaultFeatureGate</span>.<span style="color:#a6e22e">Enabled</span>(<span style="color:#a6e22e">kubefeatures</span>.<span style="color:#a6e22e">QOSReserved</span>) {
		<span style="color:#66d9ef">for</span> <span style="color:#a6e22e">resource</span>, <span style="color:#a6e22e">percentReserve</span> <span style="color:#f92672">:=</span> <span style="color:#66d9ef">range</span> <span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">qosReserved</span> {
			<span style="color:#66d9ef">switch</span> <span style="color:#a6e22e">resource</span> {
			<span style="color:#66d9ef">case</span> <span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">ResourceMemory</span>:
				<span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">setMemoryReserve</span>(<span style="color:#a6e22e">qosConfigs</span>, <span style="color:#a6e22e">percentReserve</span>)
			}
		}

		<span style="color:#a6e22e">updateSuccess</span> <span style="color:#f92672">:=</span> <span style="color:#66d9ef">true</span>
		<span style="color:#66d9ef">for</span> <span style="color:#a6e22e">_</span>, <span style="color:#a6e22e">config</span> <span style="color:#f92672">:=</span> <span style="color:#66d9ef">range</span> <span style="color:#a6e22e">qosConfigs</span> {
			<span style="color:#a6e22e">err</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">cgroupManager</span>.<span style="color:#a6e22e">Update</span>(<span style="color:#a6e22e">config</span>)
			<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
				<span style="color:#a6e22e">updateSuccess</span> = <span style="color:#66d9ef">false</span>
			}
		}
		<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">updateSuccess</span> {
			<span style="color:#a6e22e">klog</span>.<span style="color:#a6e22e">V</span>(<span style="color:#ae81ff">4</span>).<span style="color:#a6e22e">Infof</span>(<span style="color:#e6db74">&#34;[ContainerManager]: Updated QoS cgroup configuration&#34;</span>)
			<span style="color:#66d9ef">return</span> <span style="color:#66d9ef">nil</span>
		}

		<span style="color:#75715e">// If the resource can adjust the ResourceConfig to increase likelihood of
</span><span style="color:#75715e"></span>		<span style="color:#75715e">// success, call the adjustment function here.  Otherwise, the Update() will
</span><span style="color:#75715e"></span>		<span style="color:#75715e">// be called again with the same values.
</span><span style="color:#75715e"></span>		<span style="color:#66d9ef">for</span> <span style="color:#a6e22e">resource</span>, <span style="color:#a6e22e">percentReserve</span> <span style="color:#f92672">:=</span> <span style="color:#66d9ef">range</span> <span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">qosReserved</span> {
			<span style="color:#66d9ef">switch</span> <span style="color:#a6e22e">resource</span> {
			<span style="color:#66d9ef">case</span> <span style="color:#a6e22e">v1</span>.<span style="color:#a6e22e">ResourceMemory</span>:
				<span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">retrySetMemoryReserve</span>(<span style="color:#a6e22e">qosConfigs</span>, <span style="color:#a6e22e">percentReserve</span>)
			}
		}
	}

	<span style="color:#66d9ef">for</span> <span style="color:#a6e22e">_</span>, <span style="color:#a6e22e">config</span> <span style="color:#f92672">:=</span> <span style="color:#66d9ef">range</span> <span style="color:#a6e22e">qosConfigs</span> {
		<span style="color:#a6e22e">err</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">m</span>.<span style="color:#a6e22e">cgroupManager</span>.<span style="color:#a6e22e">Update</span>(<span style="color:#a6e22e">config</span>)
		<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
			<span style="color:#a6e22e">klog</span>.<span style="color:#a6e22e">Errorf</span>(<span style="color:#e6db74">&#34;[ContainerManager]: Failed to update QoS cgroup configuration&#34;</span>)
			<span style="color:#66d9ef">return</span> <span style="color:#a6e22e">err</span>
		}
	}

	<span style="color:#a6e22e">klog</span>.<span style="color:#a6e22e">V</span>(<span style="color:#ae81ff">4</span>).<span style="color:#a6e22e">Infof</span>(<span style="color:#e6db74">&#34;[ContainerManager]: Updated QoS cgroup configuration&#34;</span>)
	<span style="color:#66d9ef">return</span> <span style="color:#66d9ef">nil</span>
}

</code></pre></div><p>该流程保证了 kubepod 相关meta配置不被串改。</p>
<h3 id="pod-level">Pod level</h3>
<p>上两层的 cgroup 配置 大多属于模块划分相关的内容， Pod level 的 cgroup 配置 则更接近于 k8s 需要着重了解的一层。</p>

  
    
  
  
    
  
  
    
  
  
    
  


<figure class="highlight golang language-golang">
  <figcaption>
    
      <span>kubelet.go</span><a href="https://github.com/kubernetes/kubernetes/blob/v1.17.0/pkg/kubelet/kubelet.go#L1617" target="_blank" rel="external">pkg/kubelet/kueblet.go</a>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-golang"><code class="golang">// Create Cgroups for the pod and apply resource parameters
// to them if cgroups-per-qos flag is enabled.
// TODO(yuhua): pod cgroup 创建
pcm := kl.containerManager.NewPodContainerManager()
// If pod has already been terminated then we need not create
// or update the pod&#39;s cgroup
if !kl.podIsTerminated(pod) {
   // When the kubelet is restarted with the cgroups-per-qos
   // flag enabled, all the pod&#39;s running containers
   // should be killed intermittently and brought back up
   // under the qos cgroup hierarchy.
   // Check if this is the pod&#39;s first sync
   firstSync := true
   for _, containerStatus := range apiPodStatus.ContainerStatuses {
      if containerStatus.State.Running != nil {
         firstSync = false
         break
      }
   }
   // Don&#39;t kill containers in pod if pod&#39;s cgroups already
   // exists or the pod is running for the first time
   podKilled := false
   if !pcm.Exists(pod) &amp;&amp; !firstSync {
      if err := kl.killPod(pod, nil, podStatus, nil); err == nil {
         podKilled = true
      }
   }
   // Create and Update pod&#39;s Cgroups
   // Don&#39;t create cgroups for run once pod if it was killed above
   // The current policy is not to restart the run once pods when
   // the kubelet is restarted with the new flag as run once pods are
   // expected to run only once and if the kubelet is restarted then
   // they are not expected to run again.
   // We don&#39;t create and apply updates to cgroup if its a run once pod and was killed above
   if !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) {
      if !pcm.Exists(pod) {
         if err := kl.containerManager.UpdateQOSCgroups(); err != nil {
            klog.V(2).Infof(&#34;Failed to update QoS cgroups while syncing pod: %v&#34;, err)
         }
         if err := pcm.EnsureExists(pod); err != nil {
            kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, &#34;unable to ensure pod container exists: %v&#34;, err)
            return fmt.Errorf(&#34;failed to ensure that the pod: %v cgroups exist and are correctly applied: %v&#34;, pod.UID, err)
         }
      }
   }
}</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>
<p>上述流程我们需要关注的是 如下几个流程</p>
<blockquote>
<p>syncPod  -&gt; kl.containerManager.NewPodContainerManager() -&gt; pcm.Exists(pod) -&gt; kl.containerManager.UpdateQOSCgroups()</p>
</blockquote>
<p><code>NewPodContainerManager</code> 并没有实质性的cgroup操作，紧跟着的判断 <code>Exists(pod)</code>-&gt; <code>GetPodContainerName</code>函数调用会尝试获取 当前 pod应当存在的 cgroup路径。并检查 pod 对应cgroup的存在。</p>
<p>如果发现不存在  对应cgroup 则进入创建流程 【创建cgroup 发生在 pod其他资源创建前】，  即 <code>UpdateQOSChroups</code>:</p>
<p>还函数会进行：</p>
<ul>
<li><code>setCPUCgroupConfig</code>  比如说保证 BestEffort  pod cpu share =2; 计算 Burst.slice 路径 cpu shares 设置应该为  <strong>所有 burstable_pod_CPU_request</strong> 的和。</li>
</ul>
<ul>
<li><code>setMemoryReserve</code> 取决于是否开启了特性功能 <strong>QOSReserved</strong>, 该功能会为  <strong>Burstable</strong> 和 <strong>BestEffort</strong> pod slice 设置 可用内存上限， 基于实时计算的各个类型的 pod的 内存使用状况。  【暂定默认关闭】</li>
<li><code>setHugePagesConfig</code> 根据功能开关设置 hugepage 用量。【暂定默认关闭】</li>
</ul>
<p>上述动作完成后 将会执行 pod 粒度 的 cgroup  创建， 更新操作 <code>EnsureExists</code></p>

  
    
  
  
    
  
  
    
  
  
    
  


<figure class="highlight golang language-golang">
  <figcaption>
    
      <span>pod_container_manager_linux.go</span><a href="https://github.com/kubernetes/kubernetes/blob/v1.17.0/pkg/kubelet/cm/pod_container_manager_linux.go#L76" target="_blank" rel="external">pkg/kubelet/cm/pod_container_manager_linux.go#L76</a>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-golang"><code class="golang">// EnsureExists takes a pod as argument and makes sure that
// pod cgroup exists if qos cgroup hierarchy flag is enabled.
// If the pod level container doesn&#39;t already exist it is created.
func (m *podContainerManagerImpl) EnsureExists(pod *v1.Pod) error {
	podContainerName, _ := m.GetPodContainerName(pod)
	// check if container already exist
	alreadyExists := m.Exists(pod)
	if !alreadyExists {
		// Create the pod container
		containerConfig := &amp;CgroupConfig{
			Name:               podContainerName,
			ResourceParameters: ResourceConfigForPod(pod, m.enforceCPULimits, m.cpuCFSQuotaPeriod),
		}
		if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.SupportPodPidsLimit) &amp;&amp; m.podPidsLimit &gt; 0 {
			containerConfig.ResourceParameters.PidsLimit = &amp;m.podPidsLimit
		}
		if err := m.cgroupManager.Create(containerConfig); err != nil {
			return fmt.Errorf(&#34;failed to create container for %v : %v&#34;, podContainerName, err)
		}
	}
	// Apply appropriate resource limits on the pod container
	// Top level qos containers limits are not updated
	// until we figure how to maintain the desired state in the kubelet.
	// Because maintaining the desired state is difficult without checkpointing.
	if err := m.applyLimits(pod); err != nil {
		return fmt.Errorf(&#34;failed to apply resource limits on container for %v : %v&#34;, podContainerName, err)
	}
	return nil
}</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>
<p>上述代码中 <code>m.cgroupManager.Create(containerConfig</code> 可以完成 pod 级别的 cgroup创建， 至此我们的 pod cgroup container 已经初具雏形。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#f92672">[</span>root@node1<span style="color:#f92672">]</span><span style="color:#75715e"># tree /sys/fs/cgroup/cpu -d</span>
/sys/fs/cgroup/cpu
├── kubepods
│   ├── besteffort
│   │   ├── pode098d78945a4d359594f9c27066aa202
│   │   │   ├── 5fca7a73d4b5c7e7a6c35415e2bfeb5533c5fc1d1b4aa80bc4cb641213ee29a3
│   │   │   └── b10831a7a2cf4e96ff6b186396e9e393e5b02aa8c447e988cc6cca5172fc5c89
│   │   └── podfc7fbc35-bb79-4a33-80a0-371d438f221e
│   │       ├── 07703a4a76b926c8432c3a8ab50b69b8dfd2891733a09d830fc1a08f8a8e0a1c
│   │       └── 3a7550ae2bd7784bc7f74fa0288d361e6d569595a0ba237bb63cdd5468073316
│   └── burstable
│       ├── pod420984c2d6d62f72216bba6857bc368b
│       │   ├── 5f734d0677fb4dd0f3e3dd3647be6ad19ec728fd3e28860c6023ea6efb7fc331
│       │   └── 93cd418eaddf86be58b200c109674a36cc04fee0eccfae4a7838a1b0e6a4f978
│       ├── pod4ebb633e-f8ba-43ee-94c7-1cf8c9105555
│       │   └── 1bc40d00174d3ea84f4fce14d734a45115e72e4af394bd7d684d9691e7749995
│       ├── podcd4bc68a-faf3-4c08-8d07-57d25a68ee1b
│       │   ├── 62ba8b7c6e1afc62d16b65e0d0ae9f82254260815025ecb6c863fa82c9ea5e8e
│       │   └── c7aca0c0fdf230a6378e3325f03242ff561ec95e7f62b3b852f2877af0235792
│       ├── podd4f2a7d434e44edd8e4a0960111bda9f
│       │   ├── 08d95544fcd3a5631c5a6a550d42bcddf8319cfb1fbe7f2482d969fc19356466
│       │   └── a7a39f33fddce54c64c4a6d0bf4b499fe3d6b3d7c7f5fc0d54445ade5cad24b1
│       ├── pode8486b59c2c8408b07026a560746b02c
│       │   ├── b9fb9af9f1c5991786c6457f5b33c90ee8e33a3d68605d409b7a2c30d5565699
│       │   └── f7f6773069032faa45ef3fc62fe337127fe40795ebc3758d03e149376d18d3da
│       ├── pode86c3e73-a96e-4ae8-9ff6-fd401cf5c9aa
│       │   ├── 4ad31b5ea0ad5743eeb67628c3bf9275d2131f2132a4bbe7081c05f63c6604ea
│       │   └── 5f31deed41c58dbc0a0530b964926968d272e846cf9d91452c40797ace7fb90a
│       └── podf27baf8c-6c71-4f3a-9445-0c63eb33d586
│           ├── 1ae1d0d1fb0d44652c8ad8ef2d782628ec04a7b0e6e5718ea6788af178505ba2
│           └── 5e898be28b1cb5ccb7f9f52eddee06c114d7d42b951b256da5544128093216be
├── machine.slice
│   └── machine-qemu<span style="color:#ae81ff">\\</span>x2d18<span style="color:#ae81ff">\\</span>x2dvm123.scope
│       ├── emulator
│       └── vcpu0
├── system.slice
└── user.slice
</code></pre></div><blockquote>
<p><code>machine.slice</code> 是 libvirt 针对每个 qemu 进程生成的 cgroup管理空间.  我们意在将其纳入 k8s cgoup 统计范围内。</p>
</blockquote>
<h3 id="container-level">Container level</h3>
<p>上述 Node level 发生在 kubelet 的 SyncPod 函数执行过程中， 同样的， container 相关的 cgroup创建 也是在这之后。</p>
<p>故事仍然要从pod 创建请求开始， 从 入口处的 kubelet   <code>syncLoop</code>  -&gt; <code>syncLoopIteration</code> -&gt; <code>HandlePodAddtions</code> -&gt; <code>dispatchWork</code> -&gt; <code>UpdatePod</code> -&gt; <code>managePodLoop</code> -&gt; <code>SyncPod</code> -&gt; <code>kl.containerRuntime.SyncPod</code></p>
<p>进入 真正创建  container流程，</p>

  
    
  
  
    
  
  
    
  
  
    
  


<figure class="highlight golang language-golang">
  <figcaption>
    
      <span>kuberuntime_manager.go</span><a href="https://github.com/kubernetes/kubernetes/blob/v1.17.0/ppkg/kubelet/kuberuntime/kuberuntime_manager.go#L640" target="_blank" rel="external">pkg/kubelet/kuberuntime/kuberuntime_manager.go#L640</a>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-golang"><code class="golang">// SyncPod syncs the running pod into the desired pod by executing following steps:
//
//  1. Compute sandbox and container changes.
//  2. Kill pod sandbox if necessary.
//  3. Kill any containers that should not be running.
//  4. Create sandbox if necessary.
//  5. Create ephemeral containers.
//  6. Create init containers.
//  7. Create normal containers.
// TODO(yuhua): 创建container 之处。
func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult)</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>
<p>containerRuntime.SyncPod 主要内容涉及到 如下几个步骤</p>
<ul>
<li>计算 sandbox 和 container 变化</li>
<li>删除无用sandbox</li>
<li>删除无用 容器</li>
<li>创建沙箱</li>
<li>创建 一次性容器</li>
<li>创建 初始化容器</li>
<li>创建 其他容器。</li>
</ul>
<p>上述前4步都与 cgroup无关，这里最后的三个创建步骤 都使用了 一些公用逻辑。</p>

  
    
  
  
    
  
  
    
  
  
    
  


<figure class="highlight golang language-golang">
  <figcaption>
    
      <span>kuberuntime_manager.go</span><a href="https://github.com/kubernetes/kubernetes/blob/v1.17.0/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L780" target="_blank" rel="external">pkg/kubelet/kuberuntime/kuberuntime_manager.go#L780</a>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-golang"><code class="golang">// Helper containing boilerplate common to starting all types of containers.
// typeName is a label used to describe this type of container in log messages,
// currently: &#34;container&#34;, &#34;init container&#34; or &#34;ephemeral container&#34;
start := func(typeName string, container *v1.Container) error {
	startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name)
	result.AddSyncResult(startContainerResult)

isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff)
if isInBackOff {
	startContainerResult.Fail(err, msg)
	klog.V(4).Infof(&#34;Backing Off restarting %v %&#43;v in pod %v&#34;, typeName, container, format.Pod(pod))
	return err
}

klog.V(4).Infof(&#34;Creating %v %&#43;v in pod %v&#34;, typeName, container, format.Pod(pod))
// NOTE (aramase) podIPs are populated for single stack and dual stack clusters. Send only podIPs.
if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, podIPs); err != nil {
	startContainerResult.Fail(err, msg)
	// known errors that are logged in other places are logged at higher levels here to avoid
	// repetitive log spam
	switch {
	case err == images.ErrImagePullBackOff:
		klog.V(3).Infof(&#34;%v start failed: %v: %s&#34;, typeName, err, msg)
	default:
		utilruntime.HandleError(fmt.Errorf(&#34;%v start failed: %v: %s&#34;, typeName, err, msg))
	}
	return err
}</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>
<p>这里只需要关注 <code>startContainer</code> 相关逻辑：</p>

  
    
  
  
    
  
  
    
  
  
    
  


<figure class="highlight golang language-golang">
  <figcaption>
    
      <span>kuberuntime_manager.go</span><a href="https://github.com/kubernetes/kubernetes/blob/v1.17.0/pkg/kubelet/kuberuntime/kuberuntime_container.go#L95" target="_blank" rel="external">pkg/kubelet/kuberuntime/kuberuntime_container.go#L95</a>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-golang"><code class="golang">// startContainer starts a container and returns a message indicates why it is failed on error.
// It starts the container through the following steps:
// * pull the image
// * create the container
// * start the container
// * run the post start lifecycle hooks (if applicable)
func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error)</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>
<p>这里的 <code>podSandboxConfig</code> 包含以下字段：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#a6e22e">Linux</span>                <span style="color:#f92672">*</span><span style="color:#a6e22e">LinuxPodSandboxConfig</span> <span style="color:#e6db74">`</span><span style="color:#e6db74">protobuf:&#34;bytes,8,opt,name=linux,proto3&#34; json:&#34;linux,omitempty&#34;</span><span style="color:#e6db74">`</span>
</code></pre></div>
  
    
  
  
    
  
  
    
  
  
    
  


<figure class="highlight golang language-golang">
  <figcaption>
    
      <span>kuberuntime_manager.go</span><a href="https://github.com/kubernetes/kubernetes/blob/v1.17.0/staging/src/k8s.io/cri-api/pkg/apis/runtime/v1alpha2/api.pb.go#L796" target="_blank" rel="external">staging/src/k8s.io/cri-api/pkg/apis/runtime/v1alpha2/api.pb.go#L796</a>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-golang"><code class="golang">// LinuxPodSandboxConfig holds platform-specific configurations for Linux
// host platforms and Linux-based containers.
type LinuxPodSandboxConfig struct {
	// Parent cgroup of the PodSandbox.
	// The cgroupfs style syntax will be used, but the container runtime can
	// convert it to systemd semantics if needed.
	CgroupParent string `protobuf:&#34;bytes,1,opt,name=cgroup_parent,json=cgroupParent,proto3&#34; json:&#34;cgroup_parent,omitempty&#34;`
	// LinuxSandboxSecurityContext holds sandbox security attributes.
	SecurityContext *LinuxSandboxSecurityContext `protobuf:&#34;bytes,2,opt,name=security_context,json=securityContext,proto3&#34; json:&#34;security_context,omitempty&#34;`
	// Sysctls holds linux sysctls config for the sandbox.
	Sysctls              map[string]string `protobuf:&#34;bytes,3,rep,name=sysctls,proto3&#34; json:&#34;sysctls,omitempty&#34; protobuf_key:&#34;bytes,1,opt,name=key,proto3&#34; protobuf_val:&#34;bytes,2,opt,name=value,proto3&#34;`
	XXX_NoUnkeyedLiteral struct{}          `json:&#34;-&#34;`
	XXX_sizecache        int32             `json:&#34;-&#34;`
}</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure>
<p>这里可以看到  CgroupParent 限制了 cgroup创建的 父路径。 至此 k8s 层的cgroup创建过程结束。</p>
<p>创建结束后的  cgroup拓扑参考 Node章节结果。</p>
<h2 id="其他相关特性">其他相关特性</h2>
<h3 id="cpuset">CPUSet</h3>
<p>当 VM 与 Pod 进行混合编排， 虚拟化语义中的 vcpu pin  可以使用  cgroup 的 cpu_set 作为 功能映射</p>
<p>k8s 中 feature Gate   <code>CPUManager </code> 负责 管理  容器的 cpu set 设置  该功能 在 k8s 1.8 进入 alpha, 在 1.10 后 beta， 目前 （k8s 1.17） 仍然属于 beta 状态。</p>
<h4 id="cpu-manager工作流">CPU Manager工作流</h4>
<p>CPU Manager为满足条件的Container分配指定的CPUs时，会尽量按照CPU Topology来分配，也就是考虑CPU Affinity，按照如下的优先顺序进行CPUs选择：（Logic CPUs就是Hyperthreads）</p>
<ol>
<li>如果Container请求的Logic CPUs数量不小于单块CPU Socket中Logci CPUs数量，那么会优先把整块CPU Socket中的Logic CPUs分配给该Container。</li>
<li>如果Container剩余请求的Logic CPUs数量不小于单块物理CPU Core提供的Logic CPUs数量，那么会优先把整块物理CPU Core上的Logic CPUs分配给该Container。</li>
<li>Container剩余请求的Logic CPUs则从按照如下规则排好序的Logic CPUs列表中选择：</li>
</ol>
<ul>
<li>number of CPUs available on the same socket</li>
<li>number of CPUs available on the same core</li>
</ul>
<h4 id="discovering-cpu-topology">Discovering CPU topology</h4>
<p>CPU Manager能正常工作的前提，是发现Node上的CPU Topology，Discovery这部分工作是由cAdvisor完成的。</p>
<p>在cAdvisor的MachineInfo中通过Topology会记录cpu和mem的Topology信息。其中Topology的每个Node对象就是对应一个CPU Socket。</p>
<h4 id="创建容器">创建容器</h4>
<p>对于满足前面提到的满足static policy的Container创建时，kubelet会为其按照约定的cpu affinity来为其挑选最优的CPU Set。Container的创建时CPU Manager工作流程大致如下：</p>
<ol>
<li>
<p>Kuberuntime调用容器运行时去创建该Container。</p>
</li>
<li>
<p>Kuberuntime将该Container交给CPU Manager处理。</p>
</li>
<li>
<p>CPU Manager为Container按照static policy逻辑进行处理。</p>
</li>
<li>
<p>CPU Manager从当前Shared Pool中挑选“最佳”Set拓扑结构的CPU，对于不满足Static Policy的Contianer，则返回Shared Pool中所有CPUS组成的Set。</p>
</li>
<li>
<p>CPU Manager将对该Container的CPUs分配情况记录到Checkpoint State中，并且从Shared Pool中删除刚分配的CPUs。</p>
</li>
<li>
<p>CPU Manager再从state中读取该Container的CPU分配信息，然后通过UpdateContainerResources cRI接口将其更新到Cpuset Cgroups中，包括对于非Static Policy Container。</p>
</li>
<li>
<p>Kuberuntime调用容器运行时Start该容器。</p>
</li>
</ol>
<p>该过程入口处于 上一章节的 Container level 中的 <code>startContainer</code> 函数：

  
    
  
  
    
  
  
    
  
  
    
  


<figure class="highlight golang language-golang">
  <figcaption>
    
      <span>kuberuntime_manager.go</span><a href="https://github.com/kubernetes/kubernetes/blob/v1.17.0/pkg/kubelet/kuberuntime/kuberuntime_container.go#L134" target="_blank" rel="external">pkg/kubelet/kuberuntime/kuberuntime_container.goo#L134</a>
    
  </figcaption>
  <table>
    <tbody>
      <tr>
        <td class="gutter">
          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
        </td>
        <td class="code">
          <pre class="code-highlight language-golang"><code class="golang">	// TODO(yuhua): 设置 CPUset
	err = m.internalLifecycle.PreStartContainer(pod, container, containerID)
	if err != nil {
		s, _ := grpcstatus.FromError(err)
		m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, &#34;Internal PreStartContainer hook failed: %v&#34;, s.Message())
		return s.Message(), ErrPreStartHook
	}</code></pre>
        </td>
      </tr>
    </tbody>
  </table>
</figure></p>
<h2 id="写在最后">写在最后</h2>
<p>libvirt 已经实现了完整的 cgroup 抽象， 但是缺少完整的 cgroup 管理流程，如果想要通过 cgroup将 vm  资源抽象 并与  Pod 的资源做统一管理， 我们在前端 （kubelet） 及  对应的 后端 （VRI） 设计完整的   cgroup 操作流程。</p>
<h3 id="参考">参考:</h3>
<ol>
<li><a href="https://github.com/kubernetes/kubernetes">  k8s </a></li>
<li><a href="https://cloud.tencent.com/developer/article/1402119">  cpu manager </a></li>
<li><a href="https://libvirt.org/cgroups.html"> libvirt cgroups </a></li>
</ol>
              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">下一篇</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="http://blog.heyuhua.com/2020/01/vertical-scaling-of-pods-pod%E7%83%AD%E7%BC%A9%E6%94%BE/" data-tooltip="Vertical Scaling of Pods | pod热缩放">
              
                  <span class="hide-xs hide-sm text-small icon-mr">上一篇</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://blog.heyuhua.com/2020/01/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://blog.heyuhua.com/2020/01/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://blog.heyuhua.com/2020/01/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
                <div id="disqus_thread">
  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2020 <a href="https://github.com/v1c77">v1c77</a><br><a href="http://beian.miit.gov.cn">粤ICP备19031392号</a>. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">下一篇</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="http://blog.heyuhua.com/2020/01/vertical-scaling-of-pods-pod%E7%83%AD%E7%BC%A9%E6%94%BE/" data-tooltip="Vertical Scaling of Pods | pod热缩放">
              
                  <span class="hide-xs hide-sm text-small icon-mr">上一篇</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://blog.heyuhua.com/2020/01/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://blog.heyuhua.com/2020/01/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://blog.heyuhua.com/2020/01/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="5">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fblog.heyuhua.com%2F2020%2F01%2F%25E4%25BB%258Ekubelet-cgroup-%25E7%25AE%25A1%25E7%2590%2586%25E6%25B5%2581%25E7%25A8%258B%25E5%2585%25A5%25E6%2589%258B%2F">
          <i class="fa fa-facebook-official"></i><span>分享到 Facebook</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=http%3A%2F%2Fblog.heyuhua.com%2F2020%2F01%2F%25E4%25BB%258Ekubelet-cgroup-%25E7%25AE%25A1%25E7%2590%2586%25E6%25B5%2581%25E7%25A8%258B%25E5%2585%25A5%25E6%2589%258B%2F">
          <i class="fa fa-twitter"></i><span>分享到 Twitter</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://plus.google.com/share?url=http%3A%2F%2Fblog.heyuhua.com%2F2020%2F01%2F%25E4%25BB%258Ekubelet-cgroup-%25E7%25AE%25A1%25E7%2590%2586%25E6%25B5%2581%25E7%25A8%258B%25E5%2585%25A5%25E6%2589%258B%2F">
          <i class="fa fa-google-plus"></i><span>分享到 Google&#43;</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://www.gravatar.com/avatar/85d1ea47d48692ca0ade206fa656bd14?s=110" alt="作者的图片" />
    
    <h4 id="about-card-name">vici</h4>
    
      <div id="about-card-bio">极度选择困难. <strong>懒癌晚期</strong></div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        software enginer
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Shenzhen | Beijing | Shanghai
      </div>
    
  </div>
</div>

    

    
  
    <div id="cover" style="background-image:url('http://qiniu.heyuhua.com/India1.little.jpg!huge.webp');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="http://blog.heyuhua.com/js/script-pcw6v3xilnxydl1vddzazdverrnn9ctynvnxgwho987mfyqkuylcb1nlt.min.js"></script>


  
    <script src="http://qiniu.heyuhua.com/highlight.pack.js"></script>
  

<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
      <script>
        var disqus_config = function () {
          this.page.url = 'http:\/\/blog.heyuhua.com\/2020\/01\/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B\/';
          
            this.page.identifier = '\/2020\/01\/%E4%BB%8Ekubelet-cgroup-%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%85%A5%E6%89%8B\/'
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'vic1blog';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  




    
  </body>
</html>

